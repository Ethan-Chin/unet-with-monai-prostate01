{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import os\n",
    "# set available cuda\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1, 3\"\n",
    "\n",
    "# basic package\n",
    "import logging\n",
    "import sys\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.functional import pad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# handle .nii.gz files\n",
    "import nibabel as nib\n",
    "\n",
    "# advanced function package (medical imaging)\n",
    "from monai import config\n",
    "from monai.data import decollate_batch\n",
    "from monai.handlers import CheckpointLoader, MeanDice, StatsHandler\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    CenterSpatialCrop,\n",
    "    ScaleIntensity,\n",
    "    EnsureType,\n",
    "    Transpose,\n",
    ")\n",
    "from ignite.handlers import Checkpoint\n",
    "\n",
    "# subnet\n",
    "from unet_unit.unet_unit import unit_model\n",
    "\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.current_device())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n",
      "2\n",
      "GeForce GTX 1080\n",
      "0\n"
     ]
    }
   ],
   "metadata": {
    "executionInfo": {
     "elapsed": 625,
     "status": "ok",
     "timestamp": 1629137628158,
     "user": {
      "displayName": "Ethan Chin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk-qJ6ZGZRAscBhS06A_-zRutKbPhpbubzUYeq=s64",
      "userId": "13779612318645953442"
     },
     "user_tz": -480
    },
    "id": "wGCA20j_MHir"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## data list, data transform and data loader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# set logging info\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "# set the experiment folder and output dirs\n",
    "root_dir = \"./exp_04\"\n",
    "output_dir = os.path.join(root_dir, 'output')\n",
    "threshold_dir = os.path.join(output_dir, 'threshold')\n",
    "singlenet_dir = os.path.join(output_dir, 'single_net')\n",
    "if not os.path.exists(root_dir):\n",
    "    os.mkdir(root_dir)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "if not os.path.exists(threshold_dir):\n",
    "    os.mkdir(threshold_dir)\n",
    "if not os.path.exists(singlenet_dir):\n",
    "    os.mkdir(singlenet_dir)\n",
    "    \n",
    "data_dir = \"./dataset/working_data\"\n",
    "test_dir = os.path.join(data_dir, 'Test')\n",
    "\n",
    "images_test = sorted(glob(os.path.join(test_dir, \"case*\", \"image.nii.gz\")))\n",
    "segs_test = sorted(glob(os.path.join(test_dir, \"case*\", \"task01_seg*.nii.gz\")))\n",
    "\n",
    "imtrans_test = Compose(\n",
    "    [\n",
    "        # LoadImage(image_only=True),\n",
    "        ScaleIntensity(),\n",
    "        Transpose((2, 0, 1)),\n",
    "        # AddChannel(),\n",
    "        # CenterSpatialCrop((640, 640)),\n",
    "        # RandSpatialCrop((96, 96), random_size=False),\n",
    "        EnsureType(),\n",
    "    ]\n",
    ")\n",
    "segtrans_test = Compose(\n",
    "    [\n",
    "        # LoadImage(image_only=True),\n",
    "        Transpose((2, 0, 1)),\n",
    "        # AddChannel(),\n",
    "        # CenterSpatialCrop((640, 640)),\n",
    "        # RandSpatialCrop((96, 96), random_size=False),\n",
    "        EnsureType(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# data set loader, generates 1 img and a list of 6 segs at a time\n",
    "class evaluator_Dataset(Dataset):\n",
    "    def __init__(self, img_list, seg_list, img_transform=None, seg_transform=None):\n",
    "        self.loader = LoadImage()\n",
    "        self.img_list = img_list\n",
    "        self.seg_list = seg_list\n",
    "        self.img_transform = img_transform\n",
    "        self.seg_transform = seg_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, img_meta_data = self.loader(self.img_list[idx])\n",
    "        img = self.img_transform(img)\n",
    "        \n",
    "        seg, seg_meta_data = [], []\n",
    "        for i in range(6):\n",
    "            tmp_seg, tmp_seg_meta_data = self.loader(self.seg_list[idx*6 + i])\n",
    "            tmp_seg = self.seg_transform(tmp_seg)\n",
    "            seg.append(tmp_seg)\n",
    "            seg_meta_data.append(tmp_seg_meta_data)\n",
    "        \n",
    "        return img, seg, img_meta_data, seg_meta_data\n",
    "\n",
    "# only batch_size=1 is allowed here\n",
    "dataset_test = evaluator_Dataset(images_test, segs_test, imtrans_test, segtrans_test)\n",
    "loader_test = DataLoader(dataset_test, batch_size=1, num_workers=4, pin_memory=torch.cuda.is_available(), shuffle=False)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "executionInfo": {
     "elapsed": 31585,
     "status": "ok",
     "timestamp": 1629133099935,
     "user": {
      "displayName": "Ethan Chin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk-qJ6ZGZRAscBhS06A_-zRutKbPhpbubzUYeq=s64",
      "userId": "13779612318645953442"
     },
     "user_tz": -480
    },
    "id": "fvDlWBrdMHiu",
    "outputId": "df32b461-3467-4bea-f00d-a03fef499de9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## load models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# the evaluation device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 6 subnets\n",
    "model = [UNet(\n",
    "    dimensions=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    ").to(device) for i in range(6)]\n",
    "\n",
    "# if we have already trained them (model param saved at the log dirs)\n",
    "pretrained = True\n",
    "\n",
    "# (train and) load models\n",
    "for id in range(1, 7):\n",
    "    if not pretrained:\n",
    "        unit_model(id, device)\n",
    "    check_point_path = glob(os.path.join(root_dir, f\"./logs_{id:02d}\", \"*.pt\"))\n",
    "    if len(check_point_path) > 1:\n",
    "        raise ValueError\n",
    "    check_point_path = check_point_path[0]\n",
    "    Checkpoint.load_objects(to_load={\"net\": model[id-1]}, checkpoint=torch.load(check_point_path))"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 5042,
     "status": "ok",
     "timestamp": 1629133197211,
     "user": {
      "displayName": "Ethan Chin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk-qJ6ZGZRAscBhS06A_-zRutKbPhpbubzUYeq=s64",
      "userId": "13779612318645953442"
     },
     "user_tz": -480
    },
    "id": "8yi-uK-QMHiy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## help functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# functions to compute dice score\n",
    "def Dice_score(inputs, targets, smooth=0):\n",
    "\n",
    "    #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "    #inputs = torch.nn.functional.sigmoid(inputs)       \n",
    "    #flatten label and prediction tensors\n",
    "    intersection = (inputs * targets).sum()                            \n",
    "    dice_score = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "    return dice_score.item()\n",
    "\n",
    "# compute the dice seperately for each channel (the first dim)\n",
    "def channel_parallel_Dice_score(inputs, targets, smooth=0):\n",
    "    #Channel first input needed\n",
    "    return torch.tensor([Dice_score(i, j) for i, j in zip(torch.split(inputs, 1), torch.split(targets, 1))])\n",
    "\n",
    "\n",
    "# save nifti file; if file_name is not given, then use the one in the metadata\n",
    "def save_nifti(tensor_img, meta_data, data_dir, file_name = None):\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.mkdir(data_dir)\n",
    "    \n",
    "    case_name = meta_data['filename_or_obj'].split('/')[-2]\n",
    "    if not file_name:\n",
    "        file_name = meta_data['filename_or_obj'].split('/')[-1]\n",
    "    case_dir = os.path.join(data_dir, case_name)\n",
    "    if not os.path.exists(case_dir):\n",
    "        os.mkdir(case_dir)\n",
    "    \n",
    "    full_name = os.path.join(case_dir, file_name)\n",
    "    \n",
    "    nib.Nifti1Image(np.array(tensor_img.cpu()), np.array(meta_data['affine'])).to_filename(full_name)\n",
    "\n",
    "# create dice score record sheets\n",
    "threshold_dicesheet = pd.DataFrame(columns = [f\"{i:.1f}\" for i in (0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)], index = [i.split('/')[-2] for i in images_test])    \n",
    "single_net_dicesheet = pd.DataFrame(columns = [f\"{i:02d}\" for i in range(1, 7)], index = [i.split('/')[-2] for i in images_test])\n",
    "corr_matrix = np.zeros((6, 6))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## evaluation process"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# creat transforms\n",
    "post_trans_avgd_out = [Compose([EnsureType(), AsDiscrete(threshold_values=True, logit_thresh=i)]) for i in (0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)]\n",
    "post_trans_single_out = Compose([EnsureType(), Activations(sigmoid=True), AsDiscrete(threshold_values=True, logit_thresh=0.5)])\n",
    "cut_trans = Compose([CenterSpatialCrop((1, 640, 640)), EnsureType()])\n",
    "\n",
    "\n",
    "for i in range(6):\n",
    "    model[i].eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    # initialize mean dice\n",
    "    mean_dice=torch.zeros((1, len(post_trans_avgd_out)))\n",
    "    \n",
    "    for item, case in zip(loader_test, threshold_dicesheet.index):\n",
    "        # preprocess\n",
    "        img, segs, img_meta_data, segs_meta_data = item[0], item[1], decollate_batch(item[2])[0], decollate_batch(item[3])[0]\n",
    "        img = img.to(device)\n",
    "        segs = [i.to(device) for i in segs]\n",
    "        \n",
    "        # check if batchsize is 1\n",
    "        if img.shape[0] != 1:\n",
    "            raise ValueError\n",
    "        \n",
    "        # aggregate the results\n",
    "        output = torch.zeros((img.shape[1:])).to(device)  # the output\n",
    "        ctn_gt = torch.zeros((img.shape[1:])).to(device)  # the continuous ground truth\n",
    "\n",
    "        # for each 6 subnets, catch their singel output\n",
    "        for i in range(6):\n",
    "            # check the size whether to crop to 640, 640\n",
    "            if img.shape != (1, 1, 640, 640):\n",
    "                # singel subnet output\n",
    "                single_output = post_trans_single_out(model[i](cut_trans(img)))\n",
    "                single_output = pad(single_output, (0, 0, 160, 160), \"constant\", 0)\n",
    "            else:\n",
    "                single_output = post_trans_single_out(model[i](img))\n",
    "            \n",
    "            # cross record other experts result\n",
    "            for j in range(6):\n",
    "                corr_matrix[j, i] += Dice_score(single_output, segs[j])\n",
    "            # the matching expert\n",
    "            single_dice = Dice_score(single_output, segs[i])\n",
    "        \n",
    "            # record result and save img\n",
    "            single_net_dicesheet.loc[case, f'{i+1:02d}'] = single_dice\n",
    "            save_nifti(single_output.squeeze(dim=0).permute([1, 2, 0]), segs_meta_data[i], singlenet_dir)\n",
    "            \n",
    "            # accumulate\n",
    "            output += torch.squeeze(single_output, dim=0)\n",
    "            ctn_gt += torch.squeeze(segs[i], dim=0)\n",
    "        # take the average\n",
    "        output.div_(6)\n",
    "        ctn_gt.div_(6)\n",
    "        \n",
    "        \n",
    "        # 9 different thresholds apply to output and continuous gt\n",
    "        threshed_output = [post_trans_avgd_out[i](output) for i in range(9)]\n",
    "        threshed_gt = [post_trans_avgd_out[i](ctn_gt) for i in range(9)]\n",
    "        cated_output = torch.cat(threshed_output)\n",
    "\n",
    "        # compute the mean dice with 9 thresholds\n",
    "        cated_gt = torch.cat(threshed_gt)\n",
    "        dice_row = channel_parallel_Dice_score(cated_output, cated_gt)\n",
    "        mean_dice += dice_row\n",
    "\n",
    "        # record the dice\n",
    "        threshold_dicesheet.loc[case] = dice_row\n",
    "        \n",
    "        # the mean dice of this case\n",
    "        print(f\"the {case} mean dice:\", f\"{dice_row.mean().item():.4f}\")\n",
    "\n",
    "        # save the threshed output images and gts\n",
    "        for i in range(9):\n",
    "            save_nifti(threshed_output[i].permute([1, 2, 0]), img_meta_data, threshold_dir, f\"output_threshold=point_{i+1}.nii.gz\")\n",
    "            save_nifti(threshed_gt[i].permute([1, 2, 0]), img_meta_data, threshold_dir, f\"gt_threshold=point_{i+1}.nii.gz\")\n",
    "\n",
    "# save the recording sheet   \n",
    "single_net_dicesheet.to_csv(os.path.join(singlenet_dir, \"single_net_dicesheet.csv\"))\n",
    "threshold_dicesheet.to_csv(os.path.join(threshold_dir, \"threshold_dicesheet.csv\"))\n",
    "\n",
    "# take average and save the cross dice matrix recording\n",
    "corr_matrix = np.divide(corr_matrix, len(images_test))\n",
    "np.savetxt(os.path.join(singlenet_dir, \"corr_dice_matrix.csv\"), corr_matrix, delimiter=',')\n",
    "\n",
    "# the final score (the whole mean dice score)\n",
    "mean_dice.div_(len(images_test))\n",
    "print(\"overall mean dice:\", f\"{mean_dice.mean().item():.4f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the case47 mean dice: 0.9147\n",
      "the case48 mean dice: 0.5486\n",
      "the case49 mean dice: 0.9154\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1324,
     "status": "ok",
     "timestamp": 1629139963671,
     "user": {
      "displayName": "Ethan Chin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk-qJ6ZGZRAscBhS06A_-zRutKbPhpbubzUYeq=s64",
      "userId": "13779612318645953442"
     },
     "user_tz": -480
    },
    "id": "dRWWdVhnMHiy",
    "outputId": "724e1405-4902-4de8-d17f-c02a94a74043"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "evaluator.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "792954acd33b975aa4b1028c4fa2e0ad5e5d0bff6beb0b2977a45793440946b5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}